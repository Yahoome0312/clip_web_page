<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>1. Background - CLIP</title>
    <link rel="stylesheet" href="../assets/styles/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>

    <header>
        <nav>
            <div class="logo">CLIP</div>
            <div class="nav-links">
                <a href="../index.html">Home</a>
                <a href="section1.html" class="active">1. Background</a>
                <a href="section2.html">2. Methodology</a>
                <a href="section3.html">3. Applications</a>
                <a href="section4.html">4. Conclusion</a>
            </div>
        </nav>
    </header>

    <main>
        <h1>1. Background</h1>

        <section class="content-section">
            <h2>1.1 Standard Supervised Learning</h2>
            <p>
                Before CLIP, computer vision relied heavily on standard supervised learning. Models were trained to answer very specific questions like 
                "Which of these predefined categories does this image belong to?".
            </p>
            <p>
                The workflow was linear: <strong>Input Image &rarr; Extract Features &rarr; Predict Fixed Class Label</strong>.
            </p>
            
            <div class="visual-container" style="flex-direction: column; gap: 1rem; background: #1e293b; border: 1px solid #334155;">
                <div style="display: flex; align-items: center; gap: 1rem; flex-wrap: wrap; justify-content: center;">
                    <div style="padding: 1rem; background: #475569; border-radius: 6px;">Image</div>
                    <div style="color: var(--primary-color); font-size: 1.5rem;">&rarr;</div>
                    <div style="padding: 1rem; background: #475569; border-radius: 6px;">ResNet (Feature Extractor)</div>
                    <div style="color: var(--primary-color); font-size: 1.5rem;">&rarr;</div>
                    <div style="padding: 1rem; background: var(--secondary-color); color: #fff; border-radius: 6px;">"Dog" (Label)</div>
                </div>
                <p class="placeholder-text" style="font-size: 0.8rem; margin-top: 1rem;">Standard Supervised Flow</p>
            </div>

            <p>This approach suffers from a major flaw known as <span class="term-highlight">Ontological Rigidity<span class="term-tooltip">The inability of a model to recognize concepts outside its fixed training vocabulary (e.g., recognizing "Dog" but not "Pikachu" if not explicitly trained).</span></span>.</p>
            
            <ul>
                <li><strong>Fixed Vocabulary:</strong> If trained on ImageNet (1000 classes), it cannot recognize a "Tesla Cybertruck" or "COVID-19 Mask" simply because they weren't in the training set.</li>
                <li><strong>Loss of Semantics:</strong> To the model, a "Dalmatian" and a "German Shepherd" are just Class ID 5 and Class ID 12. It doesn't inherently understand they are both dogs.</li>
            </ul>
        </section>

        <section class="content-section">
            <h2>1.2 Weak Supervision & The Generative Trap</h2>
            <p>
                Researchers realized the internet is full of natural signals (captions, alt text). Early attempts like VirTex used a 
                <strong>Generative Objective</strong> (predicting the caption from the image).
            </p>
            <p>
                However, image descriptions have high <span class="term-highlight">Entropy<span class="term-tooltip">High variability/uncertainty. A single image can be validly described in many different ways ("A dog", "My pet Fido", "An animal on grass").</span></span>. 
                Forcing a model to predict the <em>exact</em> words of a caption is inefficient and wastes computation on non-visual words like "a" or "the".
            </p>
        </section>

        <section class="content-section">
            <h2>1.3 The Core Idea of CLIP</h2>
            <p>
                CLIP (Contrastive Language-Image Pre-training) introduced a paradigm shift. Instead of predicting exact words, it asks a simpler question:
            </p>
            <h3 style="text-align: center; margin: 2rem 0;">"Match or No Match?"</h3>
            <p>
                It learns by aligning images and text in a shared space. If an image and a text description are related, their mathematical representations (embeddings) 
                should be close together. If unrelated, they should be far apart.
            </p>
        </section>

        <div class="page-nav">
            <a href="../index.html" class="btn btn-secondary">&larr; Home</a>
            <a href="section2.html" class="btn">Next: Methodology &rarr;</a>
        </div>

    </main>

    <footer>
        <p>&copy; 2025 CLIP Project.</p>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
