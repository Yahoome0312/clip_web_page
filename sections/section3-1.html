<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3.1 Zero-Shot Classification - CLIP Applications</title>
    <link rel="stylesheet" href="../assets/styles/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\[', right: '\]', display: true}
        ]
    });"></script>
    <style>
        .classifier-demo {
            display: flex;
            gap: 2rem;
            background: #ffffff;
            padding: 2rem;
            border-radius: 12px;
            border: 1px solid #e2e8f0;
            flex-wrap: wrap;
        }
        .image-preview {
            flex: 1;
            min-width: 250px;
            text-align: center;
        }
        .image-preview img {
            width: 100%;
            max-width: 300px;
            border-radius: 8px;
            border: 2px solid #3b82f6;
            box-shadow: 0 4px 15px rgba(59, 130, 246, 0.3);
        }
        .labels-panel {
            flex: 1;
            min-width: 300px;
        }
        .label-row {
            margin-bottom: 1rem;
            position: relative;
        }
        .label-text {
            display: flex;
            justify-content: space-between;
            margin-bottom: 0.25rem;
            font-size: 0.9rem;
            font-weight: 500;
        }
        .progress-bg {
            background: #e2e8f0;
            height: 12px;
            border-radius: 6px;
            overflow: hidden;
        }
        .progress-fill {
            height: 100%;
            background: var(--primary-color);
            width: 0%;
            transition: width 1s cubic-bezier(0.4, 0, 0.2, 1);
        }
        .run-btn {
            background: var(--secondary-color);
            color: white;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 6px;
            cursor: pointer;
            font-weight: bold;
            font-size: 1rem;
            width: 100%;
            margin-top: 1rem;
            transition: transform 0.1s;
        }
        .run-btn:active { transform: scale(0.98); }
    </style>
</head>
<body>

    <header>
        <nav>
            <div class="logo">CLIP</div>
            <div class="nav-links">
                <a href="../index.html">Home</a>
                <a href="section1-1.html">1. Background</a>
                <a href="section2-1.html">2. Methodology</a>
                <a href="section3-1.html" class="active">3. Applications</a>
                <a href="section4.html">4. Conclusion</a>
            </div>
        </nav>
    </header>

    <main>
        <h1>3. Applications</h1>
        
        <div class="chapter-nav">
            <a href="section3-1.html" class="active-chapter">3.1 Zero-Shot Classification</a>
            <a href="section3-2.html">3.2 Retrieval / Search</a>
            <a href="section3-3.html">3.3 Generative Guidance</a>
        </div>

        <section class="content-section">
            <h2>3.1 Zero-Shot Classification</h2>
            <p>
                Zero-shot classification is one of the most intuitive and influential applications of CLIP, directly addressing a central limitation of traditional supervised learning: the need for labeled training data for every new task.
                CLIP does not.
                It can classify images into categories it has <strong>never seen before</strong> during training.
            </p>

            <h3>How it works</h3>
            <p>
                The procedure is straightforward. To classify an image into categories such as <em>cat</em>, <em>dog</em>, or <em>car</em>, each category is first expressed as a natural language prompt, for example:
            </p>
            <ul style="margin-left: 30px;">
                <li><em>"a photo of a cat"</em></li>
                <li><em>"a photo of a dog"</em></li>
                <li><em>"a photo of a car"</em></li>
            </ul>
            <p>
                These prompts are encoded using CLIP's text encoder, while the image is encoded using the image encoder. The predicted class is simply the text prompt whose embedding has the <strong>highest cosine similarity</strong> with the image embedding.
            </p>

            <h3 style="margin-top: 2rem;">Interactive Simulator</h3>
            <p>Click "Run Classification" to see how CLIP scores these prompts against the image.</p>
            
            <div class="classifier-demo">
                <div class="image-preview">
                    <img src="../assets/images/dog.jpg" alt="Test Image">
                    <p style="margin-top: 1rem; color: #94a3b8;">Input Image ($I$)</p>
                </div>
                <div class="labels-panel">
                    <div class="label-row">
                        <div class="label-text">
                            <span>"A photo of a pizza"</span>
                            <span class="score-val">0%</span>
                        </div>
                        <div class="progress-bg"><div class="progress-fill" data-target="2"></div></div>
                    </div>

                    <div class="label-row">
                        <div class="label-text">
                            <span>"A photo of a cute dog"</span>
                            <span class="score-val">0%</span>
                        </div>
                        <div class="progress-bg"><div class="progress-fill" data-target="98" style="background: #10b981;"></div></div>
                    </div>

                    <div class="label-row">
                        <div class="label-text">
                            <span>"A photo of a fast car"</span>
                            <span class="score-val">0%</span>
                        </div>
                        <div class="progress-bg"><div class="progress-fill" data-target="5"></div></div>
                    </div>

                    <div class="label-row">
                        <div class="label-text">
                            <span>"An image of the moon"</span>
                            <span class="score-val">0%</span>
                        </div>
                        <div class="progress-bg"><div class="progress-fill" data-target="1"></div></div>
                    </div>

                    <button class="run-btn" onclick="runSimulation()">Run Classification</button>
                </div>
            </div>

            <script>
                function runSimulation() {
                    const fills = document.querySelectorAll('.progress-fill');
                    const scores = document.querySelectorAll('.score-val');
                    
                    // Reset
                    fills.forEach(f => f.style.width = '0%');
                    scores.forEach(s => s.innerText = '0%');

                    setTimeout(() => {
                        fills.forEach((fill, index) => {
                            const target = fill.getAttribute('data-target');
                            fill.style.width = target + '%';
                            
                            // Animate number
                            let current = 0;
                            const interval = setInterval(() => {
                                if (current >= target) clearInterval(interval);
                                else {
                                    current++;
                                    scores[index].innerText = current + '%';
                                }
                            }, 10);
                        });
                    }, 100);
                }
            </script>

            <h3 style="color: var(--secondary-color); margin-top: 3rem;">CLIP vs Traditional Classifiers</h3>
            <p>
                The key difference between CLIP and traditional classifiers lies in their representation. In a standard image classifier, prediction is performed by a fixed classification head trained to output one of a predefined set of labels. CLIP removes this classification head entirely. Instead, classification is reformulated as a <strong>similarity comparison in a shared embedding space</strong>.
            </p>

            <div style="background: #ffffff; padding: 2rem; border-radius: 10px; margin: 2rem 0; border: 1px solid #e2e8f0; box-shadow: 0 2px 8px rgba(0,0,0,0.05);">
                <h4 style="text-align: center; color: var(--text-color); margin-bottom: 1.5rem;">Example: Classifying a Banana</h4>

                <div style="text-align: center; margin-bottom: 2rem;">
                    <img src="../assets/images/banana.jpg" alt="Banana" style="width: 150%; max-width: 450px; border-radius: 8px; border: 2px solid #e2e8f0; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
                    <p style="margin-top: 0.5rem; color: #64748b; font-size: 0.9rem;">While a traditional ResNet trained on ImageNet performs well on standard photos of bananas, its understanding collapses when faced with sketches, cartoons, or unusual viewpoints. <br>In contrast, CLIP maintains consistent performance across these distinct visual styles. This demonstrates that CLIP has moved beyond memorizing pixel patterns to understanding the abstract, semantic concept of a "banana," making it far more robust to real-world variations.</p>
                </div>
            </div>

            <h3 style="color: var(--secondary-color); margin-top: 2rem;">Why This Is Powerful</h3>
            <p>
                Crucially, zero-shot classification is not a special inference trick for CLIP—it is its native operating mode. During training, CLIP never learned dataset-specific classification heads. It learned to align images with natural language descriptions. Zero-shot inference simply applies this learned alignment to new tasks.
                <br>
                <br>In this sense, CLIP does not ask, “Which label does this image belong to?” but rather, “Which description best explains this image?” This shift from labels to language is what gives CLIP its flexibility and strong generalization ability.
            </p>
        </section>

        <div class="page-nav">
            <a href="../sections/section2-3.html" class="btn btn-secondary">&larr; Methodology</a>
            <a href="section3-2.html" class="btn">Next: 3.2 Retrieval &rarr;</a>
        </div>

    </main>
</body>
</html>
