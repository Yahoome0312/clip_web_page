<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>1.2 Weak Supervision - CLIP</title>
    <link rel="stylesheet" href="../assets/styles/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {
        delimiters: [
          {left: '$$', right: '$$', display: true},
          {left: '$', right: '$', display: false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\\\[', right: '\\\\]', display: true}
        ]
    });"></script>
</head>
<body>

    <header>
        <nav>
            <div class="logo">CLIP</div>
            <div class="nav-links">
                <a href="../index.html">Home</a>
                <a href="section1-1.html" class="active">1. Background</a>
                <a href="section2-1.html">2. Methodology</a>
                <a href="section3-1.html">3. Applications</a>
                <a href="section4.html">4. Conclusion</a>
            </div>
        </nav>
    </header>

    <main>
        <h1>1. Background</h1>
        
        <div class="chapter-nav">
            <a href="section1-1.html">1.1 Supervised Learning</a>
            <a href="section1-2.html" class="active-chapter">1.2 Weak Supervision</a>
            <a href="section1-3.html">1.3 CLIP Core Idea</a>
        </div>

        <section class="content-section">
            <h2>1.2 Weak Supervision and the Generative Trap</h2>
            <p>
                At the same time, researchers have long realized that the internet is full of <strong>natural signals</strong>: HTML alt text, social media captions, and news image descriptions.
                This is known as <span class="term-highlight">Natural Language Supervision<span class="term-tooltip">Training models on pairs of images and text found naturally on the web.</span></span>.
            </p>

            <p>
                Prior to CLIP, works like <strong>VirTex</strong> and <strong>ICMLM</strong> attempted to leverage this data. They typically employed a <strong>Generative Objective</strong>: training a model to look at an image and predict the corresponding text caption (Image Captioning). While appealing in principle, this strategy has several drawbacks:
            </p>

            <div style="background: #ffffff; padding: 1.5rem; border-radius: 8px; margin: 2rem 0; border-left: 4px solid var(--secondary-color); border: 1px solid #e2e8f0; border-left-width: 4px;">
                <h3>The Generative Way</h3>
                <p>
                    These models tried to <strong>generate</strong> the text caption from the image pixel-by-pixel.
                </p>
                <div style="background: #f1f5f9; padding: 1rem; border-radius: 6px; margin-top: 1rem; color: #334155; border: 1px solid #cbd5e1;">
                    Input: [Image] &rarr; Prediction: "A" &rarr; "cute" &rarr; "puppy" ...
                </div>
            </div>

            <h3 style="color: var(--accent-color);">Image Descriptions are High-Entropy</h3>
            <p>
                A single photograph can have countless correct descriptions. This makes the learning problem extremely difficult, as the model is penalized for producing reasonable descriptions that do not exactly match the reference text.
            </p>

            <div style="display: flex; gap: 2rem; align-items: center; margin-top: 2rem; flex-wrap: wrap;">
                <div style="flex: 1; min-width: 250px; text-align: center;">
                    <img src="../assets/images/dog.jpg" alt="A dog" style="width: 100%; max-width: 300px; border-radius: 8px; border: 2px solid #e2e8f0;">
                    <p style="font-size: 0.8rem; color: #64748b; margin-top: 0.5rem;">One Image, Infinite Captions</p>
                </div>

                <div style="flex: 1; min-width: 300px;">
                    <div style="background: #ffffff; padding: 1.5rem; border-radius: 8px; border: 1px solid #e2e8f0;">
                        <h4 style="color: #ef4444; margin-bottom: 1rem;">All Equally Valid Descriptions:</h4>
                        <ul style="list-style: none; padding: 0; margin: 0;">
                            <li style="padding: 0.75rem; border-bottom: 1px solid #f1f5f9; color: #10b981; font-size: 1rem;">✔ "A cute puppy"</li>
                            <li style="padding: 0.75rem; border-bottom: 1px solid #f1f5f9; color: #10b981; font-size: 1rem;">✔ "My pet Fido"</li>
                            <li style="padding: 0.75rem; color: #10b981; font-size: 1rem;">✔ "An animal on the grass"</li>
                        </ul>
                        <p style="margin-top: 1rem; font-size: 0.9rem; color: #64748b; font-style: italic;">The model gets penalized if its valid caption doesn't match the reference.</p>
                    </div>
                </div>
            </div>

            <h3 style="margin-top: 2rem; color: var(--accent-color);">Difficulties in Resource Allocation</h3>
            <p>
                Forcing a model to predict these diverse descriptions word-for-word is extremely difficult and computationally inefficient. The model wastes vast amounts of compute learning to predict insignificant words like "a" or "the," rather than focusing on visual semantics.
            </p>
        </section>

        <div class="page-nav">
            <a href="section1-1.html" class="btn btn-secondary">&larr; Previous</a>
            <a href="section1-3.html" class="btn">Next: 1.3 CLIP Core Idea &rarr;</a>
        </div>

    </main>
</body>
</html>